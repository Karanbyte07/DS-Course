{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f68d309-68fb-45a2-b4f6-a4594ece8533",
   "metadata": {},
   "source": [
    "# Coders of Bangalore\n",
    "You ask Sam Altman for a 2.2 billion USD in funding and he makes fun of you in public\n",
    "\n",
    "He asks you where you live and finally after talking to you, gives you a challenging task \n",
    "\n",
    "Collect raw instagram data of all openai followers\n",
    "\n",
    "Answer these questions:\n",
    "\n",
    "- Who has maximum posts\n",
    "- Who has maximum followers \n",
    "- Who follows maximum people \n",
    "- How many categories (Digital creators, Non profit foundation etc do we have) of how many people do we have?\n",
    "\n",
    "You have 24 hours \n",
    "\n",
    "You then ask for data - he laughs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766163c-d547-4258-8a7e-2261cb1241ac",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "You hire Vijyalaxmi Iyer and Sam Chandra who are from Hebbal and HSR Layout respectively\n",
    "\n",
    "HSR layout and Hebbal are far away so you set up a meeting at Rameshwaram cafe, Indiranagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096cff20-5c11-4b51-b424-b84c908d7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1043b00-1f63-410c-a0b7-c365b4621038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file containing raw Instagram-like data\n",
    "# utf-8 encoding is used to avoid errors with special characters\n",
    "with open(\"massivedata.txt\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299c3bd9-9478-4ae3-89a0-0ec31bf3b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into chunks using double newline as separator\n",
    "# Each chunk represents one profile\n",
    "chunks = data.split(\"\\n\\n\")\n",
    "chunks = [c for c in chunks if len(c) > 3]  # Remove very small or empty chunks\n",
    "# print(chunks) # All cleaned chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a268a7-f199-4220-93a2-09e2684efde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloud_dev_blr\\n210 posts\\n4,500 followers\\n60 following\\nNaveen DevOps\\nEngineer\\nâ˜ï¸ AWS | Docker | K8s\\nðŸ“ Sarjapur\\ngithub.com/naveencloud'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[3]  # View 4th profile data (index starts from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931adaed-7fe9-4776-9cc4-ee2177d694fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse one chunk (one profile)\n",
    "def parse_chunk(chunk):\n",
    "    chunk = chunk.strip()  # Remove extra spaces/newlines\n",
    "    sep_chunk = chunk.split(\"\\n\")  # Split chunk into individual lines\n",
    "\n",
    "    # Assign values based on fixed line positions\n",
    "    username = sep_chunk[0]\n",
    "\n",
    "    # 105 posts - so we will remove the posts part, only number will be there , and in followers and following also\n",
    "    no_of_posts = int(\n",
    "        sep_chunk[1].split(\" post\")[0].replace(\",\", \"\")\n",
    "    )  # after split [\"165\", \"s\"] we need first[0] so 165 comes\n",
    "\n",
    "    no_of_followers = float(\n",
    "        sep_chunk[2]\n",
    "        .split(\" followers\")[0]\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"K\", \"\")\n",
    "        .replace(\"M\", \"\")\n",
    "    )\n",
    "    if \"K\" in sep_chunk[2]:\n",
    "        no_of_followers = int(no_of_followers * 1000)\n",
    "    elif \"M\" in sep_chunk[2]:\n",
    "        no_of_followers = int(no_of_followers * 1000000)\n",
    "    else:\n",
    "        no_of_followers = int(no_of_followers)\n",
    "\n",
    "    no_of_following = int(sep_chunk[3].split(\" following\")[0].replace(\",\", \"\"))\n",
    "    name = sep_chunk[4]\n",
    "\n",
    "    # If type of page and bio exist\n",
    "    if len(sep_chunk) > 5:  # agar kisi me type of page aur bio na likha ho toh\n",
    "        type_of_page = sep_chunk[5]\n",
    "        bio = \"\\n\".join(sep_chunk[6:])  # Bio can be multi-line\n",
    "    else:\n",
    "        type_of_page = \"Unknown\"  # If type or bio is missing\n",
    "        bio = \"\"\n",
    "\n",
    "    # print(username, no_of_posts, no_of_followers,no_of_following,name, type_of_page, bio, sep = \"\\n\")\n",
    "    # Return extracted data as dictionary\n",
    "    return {\n",
    "        \"Username\": username,\n",
    "        \"No_of_posts\": no_of_posts,\n",
    "        \"No_of_followers\": no_of_followers,\n",
    "        \"No_of_following\": no_of_following,\n",
    "        \"Name\": name,\n",
    "        \"Type_of_page\": type_of_page,\n",
    "        \"Bio\": bio,\n",
    "    }\n",
    "\n",
    "\n",
    "# parse_chunk(chunks[3]) # Test function on a single chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54cb56b9-ea63-4a77-b0c4-67d67a8dddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all chunks and store in a list\n",
    "all_chunks = []\n",
    "for chunk in chunks:\n",
    "    parsed_chunk = parse_chunk(chunk)\n",
    "    all_chunks.append(parsed_chunk)\n",
    "# print(all_chunks) # Check final parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a391dea7-f68e-4c9d-b8d7-8dfc8e749c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps(all_chunks, indent=4)\n",
    "with open(\"parsed_data\", \"w\") as file:\n",
    "    file.write(data)  # createa a parsed data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe07981-1d77-4f4f-8cf8-2fb207cdb3f3",
   "metadata": {},
   "source": [
    "## who has the maximum posts?\n",
    "Program is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa22b5f5-e4c9-4abf-90bc-92790f9832fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startuphub_blr\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for chunk in all_chunks:\n",
    "    if max < chunk[\"No_of_posts\"]:\n",
    "        max = chunk[\"No_of_posts\"]\n",
    "        maxi_posts = chunk  # update max\n",
    "print(maxi_posts[\"Username\"])  # us chunk ke username ko print kar do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97ed61-b0b0-4053-8960-3750e9537b90",
   "metadata": {},
   "source": [
    "## who has maximum followers?\n",
    "code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56675023-e1d0-4414-9133-8298ea10328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_anujsinghal\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for chunk in all_chunks:\n",
    "    if max < chunk[\"No_of_followers\"]:\n",
    "        max = chunk[\"No_of_followers\"]\n",
    "        maxi_followers = chunk\n",
    "print(maxi_followers[\"Username\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fdb59-a735-48c3-8fbb-34fbf5bf5567",
   "metadata": {},
   "source": [
    "## who follows maximum people?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5384902c-2e1c-438d-8e27-443682991832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bangalore_engineers_diary\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for chunk in all_chunks:\n",
    "    if max < chunk[\"No_of_following\"]:\n",
    "        max = chunk[\"No_of_following\"]\n",
    "        maxi_follwing = chunk\n",
    "print(chunk[\"Username\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bfcaa-059f-4508-8d02-f4a202dea516",
   "metadata": {},
   "source": [
    "## how many categories we do have in data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bcf355f-c6f8-4ee2-a689-057799f24666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 categories in the data!\n"
     ]
    }
   ],
   "source": [
    "categories = set()\n",
    "for chunk in all_chunks:\n",
    "    categories.add(chunk[\"Type_of_page\"])\n",
    "\n",
    "print(f\"There are {len(categories)} categories in the data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5495eef-2294-4b98-8948-bbd5de739c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
